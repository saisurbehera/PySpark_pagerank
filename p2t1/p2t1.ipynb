{"cells":[{"cell_type":"code","execution_count":1,"id":"c697aa5e","metadata":{},"outputs":[],"source":["import os\n","os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.12:0.14.0 pyspark-shell'"]},{"cell_type":"code","execution_count":2,"id":"73cc2afa","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[":: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"]},{"name":"stderr","output_type":"stream","text":["Ivy Default Cache set to: /root/.ivy2/cache\n","The jars for the packages stored in: /root/.ivy2/jars\n","com.databricks#spark-xml_2.12 added as a dependency\n",":: resolving dependencies :: org.apache.spark#spark-submit-parent-4d6e5af1-fe3d-4e3f-a852-ad2ffd542f41;1.0\n","\tconfs: [default]\n","\tfound com.databricks#spark-xml_2.12;0.14.0 in central\n","\tfound commons-io#commons-io;2.8.0 in central\n","\tfound org.glassfish.jaxb#txw2;2.3.4 in central\n","\tfound org.apache.ws.xmlschema#xmlschema-core;2.2.5 in central\n",":: resolution report :: resolve 320ms :: artifacts dl 8ms\n","\t:: modules in use:\n","\tcom.databricks#spark-xml_2.12;0.14.0 from central in [default]\n","\tcommons-io#commons-io;2.8.0 from central in [default]\n","\torg.apache.ws.xmlschema#xmlschema-core;2.2.5 from central in [default]\n","\torg.glassfish.jaxb#txw2;2.3.4 from central in [default]\n","\t---------------------------------------------------------------------\n","\t|                  |            modules            ||   artifacts   |\n","\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n","\t---------------------------------------------------------------------\n","\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n","\t---------------------------------------------------------------------\n",":: retrieving :: org.apache.spark#spark-submit-parent-4d6e5af1-fe3d-4e3f-a852-ad2ffd542f41\n","\tconfs: [default]\n","\t0 artifacts copied, 4 already retrieved (0kB/9ms)\n","Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","22/05/01 16:04:40 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n","22/05/01 16:04:40 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n","22/05/01 16:04:40 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n","22/05/01 16:04:40 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n","22/05/01 16:04:42 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.12-0.14.0.jar added multiple times to distributed cache.\n","22/05/01 16:04:42 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/commons-io_commons-io-2.8.0.jar added multiple times to distributed cache.\n","22/05/01 16:04:42 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.glassfish.jaxb_txw2-2.3.4.jar added multiple times to distributed cache.\n","22/05/01 16:04:42 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.2.5.jar added multiple times to distributed cache.\n"]}],"source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName(\"Streamming_assignment\").getOrCreate()"]},{"cell_type":"code","execution_count":3,"id":"a3985d7d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 14 items\n","drwxr-xr-x   - root   hadoop          0 2022-05-01 16:04 /user/root/.sparkStaging\n","drwxr-xr-x   - root   hadoop          0 2022-05-01 15:47 /user/root/commits\n","drwxr-xr-x   - saisur hadoop          0 2022-05-01 15:50 /user/root/h2p3\n","drwxr-xr-x   - saisur hadoop          0 2022-05-01 15:59 /user/root/h2p3_m\n","-rw-r--r--   2 root   hadoop         45 2022-05-01 15:47 /user/root/metadata\n","drwxr-xr-x   - root   hadoop          0 2022-05-01 15:47 /user/root/offsets\n","drwxr-xr-x   - root   hadoop          0 2022-04-30 01:35 /user/root/p1t2_large\n","drwxr-xr-x   - root   hadoop          0 2022-04-30 17:21 /user/root/p1t2_small\n","drwxr-xr-x   - root   hadoop          0 2022-04-30 03:30 /user/root/p1t2_test_split\n","drwxr-xr-x   - root   hadoop          0 2022-04-30 18:53 /user/root/pagerank_large\n","drwxr-xr-x   - root   hadoop          0 2022-04-30 17:47 /user/root/pagerank_small\n","drwxr-xr-x   - root   hadoop          0 2022-04-30 00:10 /user/root/part2_testq2\n","drwxr-xr-x   - root   hadoop          0 2022-05-01 15:47 /user/root/sources\n","drwxr-xr-x   - root   hadoop          0 2022-05-01 15:47 /user/root/user\n"]}],"source":["!hdfs dfs -ls /user/root"]},{"cell_type":"code","execution_count":4,"id":"a322ff2f","metadata":{},"outputs":[],"source":["from pyspark.sql.types import StructType\n","\n","userSchema = StructType().add(\"article\", \"string\").add(\"rank\", \"double\")\n","csvDF = spark.readStream \\\n","    .option(\"sep\", \"\\t\") \\\n","    .schema(userSchema) \\\n","    .csv(\"hdfs:/user/root/pagerank_large\")"]},{"cell_type":"code","execution_count":5,"id":"62e5eefa","metadata":{},"outputs":[],"source":["df = csvDF.select('article', 'rank').where(\"rank > 0.5\")"]},{"cell_type":"code","execution_count":6,"id":"256e839c","metadata":{},"outputs":[],"source":["# hadoop fs -mkdir /user/root/h2p3"]},{"cell_type":"code","execution_count":7,"id":"39927f36","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/05/01 16:04:53 WARN org.apache.spark.sql.streaming.StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"]},{"data":{"text/plain":["<pyspark.sql.streaming.StreamingQuery at 0x7ff18dcb3a00>"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["df.writeStream \\\n","    .format(\"csv\") \\\n","    .option(\"delimiter\", \"\\t\") \\\n","    .option(\"checkpointLocation\", \"hdfs:/user/root/h2p3_m/\") \\\n","    .option(\"path\", \"/user/root/h2p3_m/\") \\\n","    .start()"]},{"cell_type":"code","execution_count":11,"id":"82156100","metadata":{},"outputs":[],"source":["df.createOrReplaceTempView(\"updates\")\n","counts = spark.sql(\"select count(*) from updates\") "]},{"cell_type":"code","execution_count":null,"id":"86322187","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/05/01 16:08:28 WARN org.apache.spark.sql.streaming.StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-4f8a7c4f-c6b2-4a46-ad93-3095f007f89c. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n","22/05/01 16:08:28 WARN org.apache.spark.sql.streaming.StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["-------------------------------------------\n","Batch: 0\n","-------------------------------------------\n","+--------+\n","|count(1)|\n","+--------+\n","| 1634800|\n","+--------+\n","\n"]}],"source":["query = counts \\\n","    .writeStream \\\n","    .outputMode(\"complete\") \\\n","    .format(\"console\") \\\n","    .start()\n","\n","query.awaitTermination()"]},{"cell_type":"code","execution_count":null,"id":"349e4b6e","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":5}